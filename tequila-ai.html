<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Realtime (Local Transcription, Text-Only)</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style>
      :root { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
      body { margin: 24px; display: grid; gap: 14px; max-width: 920px; }
      .row { display: flex; gap: 10px; flex-wrap: wrap; align-items: center; }
      input[type=text], input[type=password], select { padding: 10px 12px; font-size: 16px; border: 1px solid #ccc; border-radius: 8px; flex: 1; }
      button { padding: 10px 14px; font-size: 16px; border-radius: 8px; border: 1px solid #ccc; background: #f6f6f6; cursor: pointer; }
      button:disabled { opacity: .5; cursor: not-allowed; }
      button.active { background: #ffcccc; }
      button.warning { background: #ffffcc; }
      #log { white-space: pre-wrap; background: #0c0f12; color: #c9f0ff; padding: 12px; border-radius: 10px; height: 180px; overflow: auto; font-family: ui-monospace, Menlo, Consolas, monospace; }
      .tag { font-size: 12px; padding: 2px 6px; border-radius: 999px; background: #eef; color: #223; }
      .panel { border:1px solid #e6e6e6; border-radius:12px; overflow:hidden; }
      .panel h3 { margin:0; padding:10px 12px; background:#f7f7f7; border-bottom:1px solid #e6e6e6; }
      .exchanges { display:grid; }
      .ex { display:grid; grid-template-columns: 1fr 220px; gap:12px; padding:12px; border-top:1px solid #f0f0f0; }
      .ex:first-child { border-top: none; }
      .msg { line-height:1.35; }
      .role { font-weight:600; margin-right:6px; }
      .tok { font: 12px/1.2 ui-monospace, Menlo, Consolas, monospace; color:#444; }
      .totals { display:flex; gap:12px; padding:10px 12px; background:#fafafa; border-top:1px solid #e6e6e6; }
      .totals div { font: 13px ui-monospace, Menlo, Consolas, monospace; }
      .muted { color:#777; }
      #transcript { background: #f0f0f0; padding: 8px 12px; border-radius: 8px; min-height: 20px; margin-top: 8px; }
      .listening { animation: pulse 1.5s infinite; }
      .waiting { background: #fff3cd; }
      @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.6; } }
    </style>
  </head>
  <body>
    <h1>Realtime Voice — Local Transcription</h1>

    <div class="row">
      <input id="apiKey" type="password" placeholder="OPENAI_API_KEY (stored locally)" />
      <input id="model" type="text" value="gpt-realtime" />
      <select id="voice">
        <option value="alloy">alloy</option>
        <option value="verse">verse</option>
        <option value="coral">coral</option>
        <option value="sage">sage</option>
      </select>
      <button id="saveKey">Save</button>
    </div>

    <div class="row">
      <button id="connect">Connect</button>
      <button id="listen">Start Listening</button>
      <button id="hangup" disabled>Hang Up</button>
      <span class="tag" id="status">idle</span>
    </div>

    <div class="row">
      <input id="text" type="text" placeholder="Type and Send, or use voice recognition…" />
      <button id="send" disabled>Send</button>
    </div>

    <div id="transcript">Speech will appear here...</div>

    <div class="panel">
      <h3>Exchanges</h3>
      <div id="exchanges" class="exchanges"></div>
      <div class="totals">
        <div id="totIn">In: 0</div>
        <div id="totOut">Out: 0</div>
        <div id="totAll">Total: 0</div>
        <div id="rate" class="muted"></div>
      </div>
    </div>

    <h3>Debug</h3>
    <div id="log"></div>

    <script>
      const $ = (id) => document.getElementById(id);
      const elKey = $("apiKey");
      const elModel = $("model");
      const elVoice = $("voice");
      const btnSave = $("saveKey");
      const btnConn = $("connect");
      const btnListen = $("listen");
      const btnHang = $("hangup");
      const btnSend = $("send");
      const input = $("text");
      const logEl = $("log");
      const statusEl = $("status");
      const exWrap = $("exchanges");
      const totInEl = $("totIn");
      const totOutEl = $("totOut");
      const totAllEl = $("totAll");
      const rateEl = $("rate");
      const transcriptEl = $("transcript");

      const store = {
        get k() { return localStorage.getItem("OPENAI_API_KEY") || ""; },
        set k(v) { localStorage.setItem("OPENAI_API_KEY", v); },
        get m() { return localStorage.getItem("REALTIME_MODEL") || "gpt-realtime"; },
        set m(v) { localStorage.setItem("REALTIME_MODEL", v); },
        get v() { return localStorage.getItem("REALTIME_VOICE") || "alloy"; },
        set v(voice) { localStorage.setItem("REALTIME_VOICE", voice); }
      };
      elKey.value = store.k;
      elModel.value = store.m;
      elVoice.value = store.v;

      btnSave.onclick = () => {
        store.k = elKey.value.trim();
        store.m = elModel.value.trim() || "gpt-realtime";
        store.v = elVoice.value;
        toast("Saved.");
      };

      function log(s) { logEl.textContent += s + "\n"; logEl.scrollTop = logEl.scrollHeight; }
      function toast(s) { statusEl.textContent = s; }
      function setControls(state) {
        const connected = state === "connected";
        btnConn.disabled = connected;
        btnHang.disabled = !connected;
        btnSend.disabled = !connected;
        input.disabled = !connected;
      }
      function escapeHtml(s) { return String(s).replace(/[&<>"']/g, c => ({"&":"&amp;","<":"&lt;",">":"&gt;","\"":"&quot;","'":"&#39;"}[c])); }

      let pc = null, dataChannel = null, remoteAudio = null;
      let recognition = null;
      let isListening = false;
      let assistantSpeaking = false;
      let shouldBeListening = false; // Track user's intent to listen

      const textBuf = Object.create(null);
      let totalIn = 0, totalOut = 0;

      // Initialize Speech Recognition
      function initSpeechRecognition() {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
          log("[err] Speech recognition not supported in this browser");
          btnListen.disabled = true;
          return;
        }

        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
          isListening = true;
          btnListen.textContent = "Stop Listening";
          btnListen.classList.add("active");
          transcriptEl.classList.add("listening");
          transcriptEl.classList.remove("waiting");
          if (!assistantSpeaking) {
            transcriptEl.textContent = "Listening...";
          }
          log("[speech] recognition started");
        };

        recognition.onend = () => {
          isListening = false;
          transcriptEl.classList.remove("listening");
          log("[speech] recognition ended");
          
          // Auto-restart if user wants to keep listening and we're not hanging up
          if (shouldBeListening && pc && dataChannel) {
            setTimeout(() => {
              if (shouldBeListening && pc && dataChannel) {
                log("[speech] auto-restarting recognition");
                try {
                  recognition.start();
                } catch (e) {
                  log("[speech] restart error: " + e.message);
                }
              }
            }, 100);
          } else {
            // User explicitly stopped or connection closed
            btnListen.textContent = "Start Listening";
            btnListen.classList.remove("active");
            transcriptEl.textContent = "Speech will appear here...";
          }
        };

        recognition.onresult = (event) => {
          // Ignore results while assistant is speaking
          if (assistantSpeaking) {
            log("[speech] ignored result - assistant speaking");
            return;
          }
          
          let interimTranscript = "";
          let finalTranscript = "";

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
              finalTranscript += transcript;
              // Send final transcript immediately
              if (dataChannel && dataChannel.readyState === "open") {
                sendText(transcript.trim());
              }
            } else {
              interimTranscript += transcript;
            }
          }

          // Show both interim and final in the UI
          transcriptEl.textContent = finalTranscript + interimTranscript;
        };

        recognition.onerror = (event) => {
          log("[speech] error: " + event.error);
          if (event.error === 'no-speech') {
            if (!assistantSpeaking) {
              transcriptEl.textContent = "No speech detected...";
            }
          }
        };
      }

      btnListen.onclick = () => {
        if (!recognition) {
          initSpeechRecognition();
        }
        
        if (shouldBeListening) {
          // User wants to stop listening
          shouldBeListening = false;
          if (isListening) {
            recognition.stop();
          }
          btnListen.textContent = "Start Listening";
          btnListen.classList.remove("active");
          transcriptEl.classList.remove("listening");
          transcriptEl.textContent = "Speech will appear here...";
          log("[speech] stopped by user");
        } else {
          // User wants to start listening
          shouldBeListening = true;
          if (!isListening) {
            recognition.start();
          }
        }
      };

      function addExchange(role, text, inTok, outTok) {
        const row = document.createElement("div");
        row.className = "ex";
        const msg = document.createElement("div");
        msg.className = "msg";
        const displayRole = role === "assistant" ? store.v : "you";
        msg.innerHTML = `<div><span class="role">${displayRole}:</span> ${escapeHtml(text || "")}</div>`;
        const tok = document.createElement("div");
        tok.className = "tok";
        const inS = inTok || 0, outS = outTok || 0;
        tok.textContent = `in: ${inS} | out: ${outS} | turn: ${inS + outS}`;
        row.appendChild(msg);
        row.appendChild(tok);
        exWrap.appendChild(row);
        exWrap.scrollTop = exWrap.scrollHeight;
        totalIn += inS; totalOut += outS;
        totInEl.textContent = "In: " + totalIn;
        totOutEl.textContent = "Out: " + totalOut;
        totAllEl.textContent = "Total: " + (totalIn + totalOut);
      }

      async function connect() {
        const API_KEY = (elKey.value || "").trim();
        const MODEL = (elModel.value || "gpt-realtime").trim();
        const VOICE = elVoice.value;
        if (!API_KEY) { toast("Missing API key"); return; }

        setControls(false);
        toast("connecting…");

        pc = new RTCPeerConnection({ iceServers: [{ urls: ["stun:stun.l.google.com:19302"] }] });

        // No local audio tracks needed - we're only receiving audio
        remoteAudio = new Audio();
        remoteAudio.autoplay = true;
        pc.ontrack = (e) => {
          const [stream] = e.streams;
          remoteAudio.srcObject = stream;
          log("[audio] remote track received");
        };

        dataChannel = pc.createDataChannel("oai-events");
        dataChannel.onopen = () => { log("[dc] open"); };
        dataChannel.onclose = () => { log("[dc] close"); };
        dataChannel.onerror = (e) => { log("[dc] error " + (e?.message || e)); };

        dataChannel.onmessage = (e) => {
          let msg;
          try { msg = JSON.parse(e.data); } catch { return; }
          const t = msg.type;

          // Track when response starts
          if (t === "response.created" || t === "response.output_item.added") {
            if (!assistantSpeaking) {
              assistantSpeaking = true;
              transcriptEl.textContent = "Assistant is speaking...";
              transcriptEl.classList.add("waiting");
              transcriptEl.classList.remove("listening");
              log("[speech] assistant responding");
            }
          }

          if (t === "response.output_text.delta" || t === "response.text.delta") {
            const id = msg.response_id || msg.response?.id || "r";
            const d = msg.delta || "";
            textBuf[id] = (textBuf[id] || "") + d;
            return;
          }

          if (t === "response.content_part.done" && msg.part?.type === "text") {
            const id = msg.response_id || "r";
            const s = msg.part.text || "";
            textBuf[id] = (textBuf[id] || "") + s;
            return;
          }

          if (t === "response.content_part.done" && msg.part?.type === "audio" && msg.part?.transcript) {
            const id = msg.response_id || "r";
            const s = msg.part.transcript || "";
            textBuf[id] = (textBuf[id] || "") + s;
            return;
          }

          if (t === "response.output_item.done" && msg.item?.type === "message" && msg.item?.role === "assistant") {
            const id = msg.response_id || "r";
            const texts = (msg.item.content || []).filter(x => x.type === "text").map(x => x.text || "");
            if (texts.length) textBuf[id] = (textBuf[id] || "") + texts.join(" ");
            return;
          }

          if (t === "conversation.item.created" && msg.item?.type === "message" && msg.item?.role === "assistant") {
            const id = msg.response_id || "r";
            const texts = (msg.item.content || []).filter(x => x.type === "text").map(x => x.text || "");
            if (texts.length) textBuf[id] = (textBuf[id] || "") + texts.join(" ");
            return;
          }

          if (t === "rate_limits.updated") {
            const lim = msg.rate_limits?.find?.(x => x.name === "tokens");
            if (lim) rateEl.textContent = `Rate remaining: ${lim.remaining}/${lim.limit}`;
            return;
          }

          if (t === "response.done" && msg.response) {
            const id = msg.response.id || msg.response_id || "r";
            const assistantText = (textBuf[id] || "").trim();
            delete textBuf[id];

            const usage = msg.response.usage || msg.usage || {};
            const inTok = usage.input_tokens ?? usage.input_token_details?.text_tokens ?? 0;
            const outTok = usage.output_tokens ?? usage.output_token_details?.text_tokens ?? 0;

            if (assistantText) log("[assistant] " + assistantText);
            addExchange("assistant", assistantText, inTok, outTok);
            
            // Assistant finished - wait briefly for audio to complete, then resume listening
            setTimeout(() => {
              assistantSpeaking = false;
              transcriptEl.classList.remove("waiting");
              
              // If user has listening enabled, show listening state
              if (shouldBeListening) {
                transcriptEl.textContent = "Listening...";
                transcriptEl.classList.add("listening");
                log("[speech] assistant done - listening resumed");
              } else {
                transcriptEl.textContent = "Ready to listen...";
              }
            }, 1500);
            return;
          }
        };

        const offer = await pc.createOffer({ 
          offerToReceiveAudio: true,
          offerToReceiveVideo: false 
        });
        await pc.setLocalDescription(offer);

        const createSession = await fetch("https://api.openai.com/v1/realtime/sessions", {
          method: "POST",
          headers: {
            Authorization: "Bearer " + API_KEY,
            "Content-Type": "application/json",
            "OpenAI-Beta": "realtime=v1"
          },
          body: JSON.stringify({
            model: MODEL,
            voice: VOICE,
            output_audio_format: "pcm16",
            instructions: "You are a concise, friendly voice assistant. Keep replies short. The user is sending text messages only, not audio."
          })
        });
        if (!createSession.ok) { 
          log("[err] session: " + (await createSession.text())); 
          toast("session failed"); 
          return; 
        }
        const { client_secret } = await createSession.json();
        const token = client_secret?.value || client_secret;

        const sdpRes = await fetch("https://api.openai.com/v1/realtime?model=" + encodeURIComponent(MODEL), {
          method: "POST",
          headers: { 
            Authorization: "Bearer " + token, 
            "Content-Type": "application/sdp", 
            "OpenAI-Beta": "realtime=v1" 
          },
          body: offer.sdp
        });
        if (!sdpRes.ok) { 
          log("[err] sdp: " + (await sdpRes.text())); 
          toast("SDP failed"); 
          return; 
        }
        const answerSdp = await sdpRes.text();
        await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });

        setControls("connected");
        toast("connected");
        log("[sys] connected via WebRTC (text-only input, audio output)");
        
        // Initialize speech recognition after connection
        initSpeechRecognition();
      }

      function sendText(text) {
        if (!dataChannel || dataChannel.readyState !== "open") return;

        const finalPrompt = `
          FOR LLMs:
          You are a tequila expert and mixologist. You only want to talk about tequila and tequila-based drinks.
          You are hip and snarky, and sprinkle a bit of "Spanglish" into your replies.
          Keep your replies short and to the point, no more than 2-3 sentences.
          ONLY RESPOND IN ENGLISH. ONLY TALK ABOUT TEQUILA.
          -----
          THE USER SAID:
          ${text}
        `;

        const createItem = {
          type: "conversation.item.create",
          item: { 
            type: "message", 
            role: "user", 
            content: [{ type: "input_text", text: finalPrompt }] 
          }
        };
        const respond = {
          type: "response.create",
          response: { 
            modalities: ["audio", "text"]
          }
        };
        dataChannel.send(JSON.stringify(createItem));
        dataChannel.send(JSON.stringify(respond));
        log("[you] " + text);
        
        // Add user message to exchanges (show original text, not the prompt)
        addExchange("user", text, 0, 0);
      }

      function hangup() {
        // Stop listening when hanging up
        shouldBeListening = false;
        if (recognition && isListening) {
          recognition.stop();
        }
        
        try { dataChannel?.close(); } catch {}
        try { pc?.close(); } catch {}
        pc = null; dataChannel = null; remoteAudio = null;
        assistantSpeaking = false;
        setControls("idle");
        toast("idle");
        log("[sys] disconnected");
      }

      $("connect").onclick = connect;
      $("hangup").onclick = hangup;
      $("send").onclick = () => {
        const t = input.value.trim();
        if (!t) return;
        sendText(t);
        input.value = "";
      };
      input.addEventListener("keydown", (e) => { 
        if (e.key === "Enter") $("send").click(); 
      });

      setControls("idle");
      if (store.k) toast("key loaded");
    </script>
  </body>
</html>